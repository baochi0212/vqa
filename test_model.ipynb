{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/vqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTConfig, BertConfig, VisionTextDualEncoderConfig, VisionTextDualEncoderModel\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "import requests\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import torch\n",
    "import sys\n",
    "# from src.dataset import *\n",
    "import torch.nn as nn\n",
    "\n",
    "# sys.path.append(\"\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "cuda_id = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ubuntu/vqa/huggingface_modules/models--Salesforce--blip-vqa-capfilt-large/snapshots/c6af15bed424cf343aab3ff3bb31417ba272923a were not used when initializing BlipModel: ['text_decoder.bert.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.output.dense.bias', 'text_decoder.bert.encoder.layer.7.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.attention.self.query.bias', 'text_decoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.attention.self.value.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.attention.self.key.bias', 'text_decoder.bert.encoder.layer.6.output.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.self.key.weight', 'text_encoder.encoder.layer.4.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.11.crossattention.output.dense.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.weight', 'text_encoder.encoder.layer.3.crossattention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.weight', 'text_encoder.encoder.layer.11.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_encoder.encoder.layer.2.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.bias', 'text_encoder.encoder.layer.0.crossattention.self.key.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.bias', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.4.attention.self.value.bias', 'text_encoder.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.4.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.key.weight', 'text_decoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.crossattention.self.query.weight', 'text_encoder.encoder.layer.7.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.bias', 'text_encoder.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.0.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.output.dense.weight', 'text_decoder.bert.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.4.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.attention.self.query.weight', 'text_encoder.encoder.layer.9.attention.self.value.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.bias', 'text_decoder.cls.predictions.decoder.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.bias', 'text_encoder.encoder.layer.10.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.weight', 'text_encoder.encoder.layer.4.attention.self.key.weight', 'text_decoder.cls.predictions.decoder.bias', 'text_encoder.encoder.layer.2.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.11.attention.self.key.bias', 'text_decoder.bert.encoder.layer.1.attention.self.key.weight', 'text_decoder.bert.encoder.layer.10.attention.self.query.weight', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.self.query.weight', 'text_encoder.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.weight', 'text_encoder.encoder.layer.7.attention.self.value.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.bias', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.0.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.10.attention.self.value.weight', 'text_encoder.encoder.layer.10.attention.self.query.weight', 'text_decoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_decoder.bert.embeddings.position_embeddings.weight', 'text_encoder.encoder.layer.8.attention.self.value.weight', 'text_decoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.intermediate.dense.bias', 'text_encoder.encoder.layer.0.attention.self.key.weight', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.attention.self.query.bias', 'text_decoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_decoder.bert.embeddings.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.self.query.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.11.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.crossattention.self.value.bias', 'text_encoder.encoder.layer.8.attention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.crossattention.self.key.bias', 'text_encoder.encoder.layer.4.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.bias', 'text_encoder.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.4.crossattention.self.query.bias', 'text_encoder.encoder.layer.5.attention.self.value.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.9.intermediate.dense.weight', 'text_encoder.embeddings.LayerNorm.weight', 'text_encoder.encoder.layer.5.crossattention.self.query.bias', 'text_encoder.encoder.layer.1.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.bias', 'text_encoder.encoder.layer.0.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.self.query.weight', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.6.attention.output.dense.bias', 'text_encoder.embeddings.word_embeddings.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.6.attention.self.value.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.bias', 'text_decoder.cls.predictions.transform.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.crossattention.self.query.weight', 'text_encoder.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.8.crossattention.self.value.weight', 'text_encoder.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.weight', 'text_encoder.encoder.layer.6.attention.self.key.weight', 'text_decoder.bert.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.5.attention.self.value.bias', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.bias', 'text_encoder.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.1.attention.self.query.bias', 'text_decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.self.query.bias', 'text_encoder.encoder.layer.7.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.8.crossattention.self.key.bias', 'text_encoder.encoder.layer.6.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.self.key.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.weight', 'text_encoder.encoder.layer.4.attention.self.key.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.attention.self.value.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.weight', 'text_decoder.bert.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.self.value.weight', 'text_encoder.encoder.layer.7.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.output.dense.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.8.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.1.attention.self.value.weight', 'text_decoder.bert.encoder.layer.5.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.attention.self.key.weight', 'text_decoder.bert.encoder.layer.6.attention.self.query.bias', 'text_encoder.encoder.layer.3.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.query.bias', 'text_encoder.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.intermediate.dense.bias', 'text_encoder.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.intermediate.dense.weight', 'text_encoder.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.0.attention.self.value.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.1.intermediate.dense.bias', 'text_encoder.encoder.layer.1.crossattention.self.value.weight', 'text_encoder.encoder.layer.9.attention.output.dense.bias', 'text_encoder.encoder.layer.5.attention.self.query.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.5.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.self.key.bias', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.weight', 'text_encoder.encoder.layer.6.attention.self.value.bias', 'text_decoder.bert.encoder.layer.5.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.8.output.dense.weight', 'text_encoder.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.9.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.2.attention.self.value.weight', 'text_encoder.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.weight', 'text_encoder.encoder.layer.1.attention.self.value.weight', 'text_encoder.encoder.layer.9.attention.self.value.weight', 'text_decoder.bert.encoder.layer.10.attention.self.value.bias', 'text_decoder.bert.encoder.layer.8.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.4.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.self.query.bias', 'text_encoder.encoder.layer.5.attention.self.key.bias', 'text_decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.output.dense.bias', 'text_encoder.encoder.layer.10.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.weight', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.self.query.weight', 'text_encoder.encoder.layer.10.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.9.output.dense.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.output.dense.bias', 'text_decoder.bert.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.self.query.weight', 'text_decoder.bert.encoder.layer.3.attention.self.key.weight', 'text_decoder.bert.encoder.layer.4.attention.self.value.bias', 'text_decoder.bert.encoder.layer.1.output.dense.bias', 'text_encoder.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.self.query.bias', 'text_encoder.encoder.layer.8.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.self.key.bias', 'text_encoder.encoder.layer.7.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.10.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.6.attention.self.value.weight', 'text_decoder.bert.encoder.layer.4.output.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.8.attention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.5.attention.self.key.weight', 'text_encoder.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.key.weight', 'text_encoder.encoder.layer.1.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.crossattention.output.dense.bias', 'text_encoder.encoder.layer.0.crossattention.self.key.weight', 'text_encoder.encoder.layer.3.attention.self.key.weight', 'text_encoder.encoder.layer.9.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.10.attention.self.key.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.weight', 'text_encoder.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.11.output.dense.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.value.bias', 'text_decoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.encoder.layer.7.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.6.crossattention.output.dense.bias', 'text_encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_encoder.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.10.intermediate.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.key.weight', 'text_encoder.encoder.layer.2.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.self.query.weight', 'text_encoder.encoder.layer.6.crossattention.self.key.weight', 'text_encoder.encoder.layer.5.crossattention.output.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.value.bias', 'text_encoder.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.2.attention.self.query.weight', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.self.query.bias', 'text_encoder.encoder.layer.0.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.output.dense.bias', 'text_decoder.bert.encoder.layer.2.output.dense.bias', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.output.dense.weight', 'text_decoder.bert.encoder.layer.6.attention.self.key.bias', 'text_encoder.encoder.layer.2.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.8.crossattention.self.query.bias', 'text_encoder.encoder.layer.6.attention.self.value.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.key.weight', 'text_decoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_encoder.encoder.layer.5.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.attention.self.value.bias', 'text_encoder.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.bias', 'text_encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.8.output.dense.bias', 'text_encoder.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.8.intermediate.dense.bias', 'text_encoder.encoder.layer.3.intermediate.dense.bias', 'text_encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'text_decoder.bert.embeddings.LayerNorm.bias', 'text_decoder.bert.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.4.crossattention.self.key.bias', 'text_encoder.encoder.layer.5.output.dense.weight', 'text_decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.attention.self.value.weight', 'text_decoder.bert.encoder.layer.5.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.1.attention.self.query.weight', 'text_encoder.encoder.layer.8.attention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.self.key.weight', 'text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_encoder.encoder.layer.11.attention.self.value.bias', 'text_decoder.bert.encoder.layer.1.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.5.attention.self.value.weight', 'text_encoder.encoder.layer.3.attention.self.value.bias', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.attention.self.value.weight', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.6.output.dense.bias', 'text_decoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.attention.self.query.bias', 'text_encoder.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.dense.weight', 'text_encoder.encoder.layer.11.intermediate.dense.bias', 'text_encoder.encoder.layer.2.attention.self.key.weight', 'text_decoder.bert.encoder.layer.11.crossattention.self.query.bias', 'text_encoder.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.embeddings.position_ids', 'text_decoder.bert.encoder.layer.8.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.attention.self.value.weight', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.9.crossattention.self.value.bias', 'text_encoder.embeddings.position_embeddings.weight', 'text_decoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_encoder.encoder.layer.1.output.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.query.bias', 'text_decoder.bert.encoder.layer.4.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.9.attention.self.value.bias', 'text_decoder.bert.encoder.layer.0.crossattention.self.query.weight', 'text_encoder.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.11.attention.self.key.weight', 'text_encoder.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.6.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.key.weight', 'text_encoder.encoder.layer.3.attention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.3.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.3.attention.self.key.bias', 'text_decoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_encoder.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.7.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.self.query.weight', 'text_encoder.encoder.layer.3.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.6.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.encoder.layer.10.attention.self.value.bias', 'text_encoder.encoder.layer.8.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.self.key.weight', 'text_encoder.encoder.layer.6.attention.self.query.bias', 'text_decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.crossattention.self.key.weight', 'text_encoder.encoder.layer.7.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.11.output.dense.bias', 'text_decoder.bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.2.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.query.bias', 'text_decoder.bert.embeddings.position_ids', 'text_encoder.encoder.layer.7.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.3.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.query.weight', 'text_encoder.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.attention.self.query.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_encoder.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.bias', 'text_encoder.encoder.layer.3.output.dense.weight', 'text_encoder.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.bias', 'text_encoder.encoder.layer.4.attention.self.query.bias', 'text_decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.attention.self.key.weight', 'text_decoder.bert.encoder.layer.11.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_encoder.encoder.layer.5.crossattention.self.value.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.2.attention.self.key.bias', 'text_decoder.bert.encoder.layer.0.attention.self.value.bias', 'text_decoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.2.crossattention.self.key.bias', 'text_encoder.encoder.layer.1.attention.self.value.bias', 'text_decoder.bert.encoder.layer.1.attention.self.key.bias', 'text_decoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_encoder.encoder.layer.0.attention.self.key.bias', 'text_decoder.bert.encoder.layer.3.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.9.attention.self.key.bias', 'text_encoder.encoder.layer.6.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.attention.self.value.weight', 'text_decoder.bert.encoder.layer.11.attention.output.dense.bias', 'text_encoder.encoder.layer.3.crossattention.self.query.weight', 'text_encoder.encoder.layer.5.crossattention.self.value.weight', 'text_encoder.encoder.layer.11.attention.output.dense.weight', 'text_encoder.encoder.layer.1.crossattention.self.query.weight', 'text_encoder.encoder.layer.8.crossattention.self.query.bias', 'text_encoder.encoder.layer.7.attention.self.key.weight', 'text_encoder.encoder.layer.1.attention.self.key.weight', 'text_encoder.encoder.layer.6.crossattention.self.value.bias', 'text_encoder.encoder.layer.4.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.7.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.6.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.query.bias', 'text_decoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.11.crossattention.self.value.weight', 'text_encoder.encoder.layer.1.crossattention.self.value.bias', 'text_encoder.encoder.layer.6.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_encoder.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.9.crossattention.output.dense.bias', 'text_encoder.encoder.layer.2.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.output.dense.bias', 'text_encoder.encoder.layer.9.crossattention.self.query.weight', 'text_encoder.encoder.layer.2.attention.self.value.bias', 'text_encoder.encoder.layer.1.intermediate.dense.weight', 'text_encoder.encoder.layer.2.crossattention.self.value.weight', 'text_encoder.encoder.layer.10.output.dense.bias', 'text_decoder.bert.encoder.layer.7.attention.self.query.weight', 'text_encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.2.crossattention.self.value.weight', 'text_decoder.bert.encoder.layer.0.crossattention.output.dense.bias', 'text_encoder.encoder.layer.4.output.dense.weight', 'text_encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.output.dense.weight', 'text_encoder.embeddings.LayerNorm.bias', 'text_encoder.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.encoder.layer.8.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.9.attention.self.query.weight', 'text_encoder.encoder.layer.4.attention.output.dense.bias', 'text_encoder.encoder.layer.6.output.dense.bias', 'text_decoder.bert.encoder.layer.6.crossattention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.self.key.weight', 'text_encoder.encoder.layer.8.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.7.crossattention.self.key.weight', 'text_encoder.encoder.layer.0.intermediate.dense.weight', 'text_decoder.bert.encoder.layer.7.output.dense.bias', 'text_encoder.encoder.layer.0.crossattention.output.dense.weight', 'text_encoder.encoder.layer.1.attention.self.key.bias', 'text_encoder.encoder.layer.7.attention.self.key.bias', 'text_encoder.encoder.layer.11.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.encoder.layer.7.crossattention.self.value.weight', 'text_encoder.encoder.layer.8.crossattention.self.query.weight', 'text_decoder.bert.encoder.layer.2.attention.self.query.bias', 'text_encoder.encoder.layer.1.attention.output.dense.bias', 'text_decoder.cls.predictions.bias', 'text_encoder.encoder.layer.1.attention.self.query.weight', 'text_decoder.bert.encoder.layer.5.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.attention.self.value.weight', 'text_encoder.encoder.layer.8.intermediate.dense.bias', 'text_decoder.cls.predictions.transform.dense.weight', 'text_decoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.encoder.layer.4.output.dense.bias', 'text_encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.10.attention.self.query.bias', 'text_encoder.encoder.layer.3.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.11.output.dense.bias', 'text_encoder.encoder.layer.7.crossattention.output.dense.bias', 'text_decoder.bert.encoder.layer.11.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.attention.self.query.weight', 'text_encoder.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.attention.self.value.bias', 'text_encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.output.dense.bias', 'text_decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.encoder.layer.0.crossattention.self.query.weight', 'text_encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.1.crossattention.self.query.weight', 'text_encoder.encoder.layer.9.intermediate.dense.weight', 'text_encoder.encoder.layer.0.attention.self.query.weight', 'text_encoder.encoder.layer.1.crossattention.self.key.weight', 'text_encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.output.dense.weight', 'text_encoder.encoder.layer.0.output.dense.weight', 'text_encoder.encoder.layer.0.attention.self.value.bias', 'text_encoder.encoder.layer.2.attention.self.key.bias', 'text_encoder.encoder.layer.6.crossattention.output.dense.weight', 'text_decoder.bert.encoder.layer.9.attention.self.value.weight', 'text_decoder.bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_decoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_encoder.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.encoder.layer.7.output.LayerNorm.weight', 'text_decoder.bert.encoder.layer.11.attention.self.key.weight', 'text_decoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_decoder.bert.encoder.layer.4.crossattention.self.key.bias', 'text_decoder.bert.encoder.layer.4.crossattention.output.dense.weight', 'text_decoder.bert.embeddings.word_embeddings.weight', 'text_decoder.bert.encoder.layer.8.attention.self.key.bias', 'text_encoder.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.encoder.layer.5.crossattention.self.query.weight', 'text_encoder.encoder.layer.10.attention.output.dense.bias', 'text_decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.encoder.layer.9.output.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BlipModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BlipModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BlipModel were not initialized from the model checkpoint at /home/ubuntu/vqa/huggingface_modules/models--Salesforce--blip-vqa-capfilt-large/snapshots/c6af15bed424cf343aab3ff3bb31417ba272923a and are newly initialized: ['text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.4.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.query.bias', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.10.crossattention.self.key.bias', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.8.crossattention.self.key.weight', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.7.crossattention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.query.bias', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'logit_scale', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.10.crossattention.output.dense.weight', 'text_model.encoder.layer.7.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.value.bias', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.self.value.weight', 'text_model.encoder.layer.6.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.output.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.embeddings.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.self.value.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.self.query.weight', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.self.query.weight', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.self.key.bias', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.9.crossattention.self.key.bias', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.9.crossattention.self.query.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.self.query.weight', 'text_model.embeddings.position_embeddings.weight', 'text_model.encoder.layer.6.crossattention.self.value.bias', 'text_model.encoder.layer.10.crossattention.self.value.weight', 'text_model.encoder.layer.2.crossattention.self.value.weight', 'text_model.encoder.layer.6.crossattention.self.query.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.9.crossattention.self.value.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.0.crossattention.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.self.key.bias', 'text_model.encoder.layer.0.attention.output.dense.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.crossattention.output.dense.weight', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.10.crossattention.self.key.weight', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.6.crossattention.self.key.weight', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.8.crossattention.self.value.weight', 'text_model.encoder.layer.8.crossattention.self.query.bias', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.4.crossattention.self.query.weight', 'text_model.encoder.layer.2.crossattention.self.key.bias', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.1.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.5.crossattention.self.query.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.query.bias', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.9.crossattention.self.query.bias', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.0.crossattention.output.dense.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.crossattention.self.value.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.9.crossattention.self.key.weight', 'text_model.encoder.layer.1.crossattention.self.value.weight', 'text_model.encoder.layer.5.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.value.weight', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.encoder.layer.5.crossattention.self.query.weight', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.pooler.dense.weight', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.5.crossattention.self.value.weight', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.pooler.dense.bias', 'text_model.encoder.layer.6.crossattention.self.key.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.0.crossattention.self.value.bias', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.4.crossattention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.self.query.bias', 'text_model.encoder.layer.6.crossattention.self.value.weight', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.6.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.7.output.dense.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.11.crossattention.self.query.bias', 'text_model.encoder.layer.2.crossattention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.encoder.layer.7.crossattention.output.dense.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.self.key.bias', 'text_model.encoder.layer.3.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.query.bias', 'text_model.encoder.layer.9.crossattention.self.value.bias', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.crossattention.self.query.weight', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.10.crossattention.self.value.bias', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.11.crossattention.self.query.weight', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.3.crossattention.self.key.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.11.crossattention.output.dense.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.5.crossattention.self.key.weight', 'text_model.encoder.layer.0.crossattention.self.query.bias', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.2.crossattention.output.dense.weight', 'text_model.encoder.layer.11.crossattention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.embeddings.LayerNorm.weight', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.1.crossattention.output.dense.weight', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.2.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.1.crossattention.self.query.weight', 'text_model.encoder.layer.7.crossattention.self.key.weight', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.3.crossattention.self.query.weight', 'text_model.encoder.layer.3.crossattention.self.query.bias', 'text_model.encoder.layer.4.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.crossattention.self.key.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.11.crossattention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.3.crossattention.output.dense.bias', 'text_model.encoder.layer.7.crossattention.output.dense.weight', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.3.crossattention.self.key.bias', 'text_model.encoder.layer.11.crossattention.self.key.bias', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.11.crossattention.self.value.weight', 'text_model.encoder.layer.10.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.1.crossattention.self.key.weight', 'text_model.encoder.layer.11.crossattention.output.LayerNorm.bias', 'text_model.encoder.layer.5.crossattention.output.dense.weight', 'text_model.encoder.layer.9.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.crossattention.self.value.weight', 'text_model.encoder.layer.6.crossattention.self.query.weight', 'text_model.encoder.layer.5.crossattention.self.key.bias', 'text_model.encoder.layer.2.crossattention.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.6.crossattention.output.dense.bias', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.1.crossattention.output.dense.bias', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.8.crossattention.output.dense.weight', 'text_model.encoder.layer.2.crossattention.self.value.bias', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.4.crossattention.self.value.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.encoder.layer.8.crossattention.output.dense.bias', 'text_model.encoder.layer.10.crossattention.output.dense.bias', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.5.crossattention.output.dense.bias', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.0.crossattention.self.value.weight', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.1.crossattention.self.key.bias', 'text_projection.weight', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.3.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.0.crossattention.self.key.weight', 'visual_projection.weight', 'text_model.encoder.layer.1.crossattention.self.value.bias', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.4.crossattention.output.dense.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.8.crossattention.output.LayerNorm.weight', 'text_model.encoder.layer.11.crossattention.self.key.weight', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_model.encoder.layer.11.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipModel, BlipForQuestionAnswering\n",
    "image_dir = \"/home/ubuntu/vqa/data/train-images\"\n",
    "text_json_path = \"/home/ubuntu/vqa/data/evjvqa_train.json\"\n",
    "\n",
    "# dataset = EVJQA(image_dir, text_json_path)\n",
    "# loader = data.DataLoader(dataset,\n",
    "#                          batch_size=32,\n",
    "#                          shuffle=True)\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--Salesforce--blip-vqa-capfilt-large/snapshots/c6af15bed424cf343aab3ff3bb31417ba272923a\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--Salesforce--blip-vqa-capfilt-large/snapshots/c6af15bed424cf343aab3ff3bb31417ba272923a\")\n",
    "vqa_model = BlipForQuestionAnswering.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--Salesforce--blip-vqa-capfilt-large/snapshots/c6af15bed424cf343aab3ff3bb31417ba272923a\").cuda(cuda_id)\n",
    "blip_model = BlipModel.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--Salesforce--blip-vqa-capfilt-large/snapshots/c6af15bed424cf343aab3ff3bb31417ba272923a\").cuda(cuda_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text_model\n",
      "text_model.embeddings\n",
      "text_model.embeddings.word_embeddings\n",
      "text_model.embeddings.position_embeddings\n",
      "text_model.embeddings.LayerNorm\n",
      "text_model.embeddings.dropout\n",
      "text_model.encoder\n",
      "text_model.encoder.layer\n",
      "text_model.encoder.layer.0\n",
      "text_model.encoder.layer.0.attention\n",
      "text_model.encoder.layer.0.attention.self\n",
      "text_model.encoder.layer.0.attention.self.query\n",
      "text_model.encoder.layer.0.attention.self.key\n",
      "text_model.encoder.layer.0.attention.self.value\n",
      "text_model.encoder.layer.0.attention.self.dropout\n",
      "text_model.encoder.layer.0.attention.output\n",
      "text_model.encoder.layer.0.attention.output.dense\n",
      "text_model.encoder.layer.0.attention.output.LayerNorm\n",
      "text_model.encoder.layer.0.attention.output.dropout\n",
      "text_model.encoder.layer.0.crossattention\n",
      "text_model.encoder.layer.0.crossattention.self\n",
      "text_model.encoder.layer.0.crossattention.self.query\n",
      "text_model.encoder.layer.0.crossattention.self.key\n",
      "text_model.encoder.layer.0.crossattention.self.value\n",
      "text_model.encoder.layer.0.crossattention.self.dropout\n",
      "text_model.encoder.layer.0.crossattention.output\n",
      "text_model.encoder.layer.0.crossattention.output.dense\n",
      "text_model.encoder.layer.0.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.0.crossattention.output.dropout\n",
      "text_model.encoder.layer.0.intermediate\n",
      "text_model.encoder.layer.0.intermediate.dense\n",
      "text_model.encoder.layer.0.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.0.output\n",
      "text_model.encoder.layer.0.output.dense\n",
      "text_model.encoder.layer.0.output.LayerNorm\n",
      "text_model.encoder.layer.0.output.dropout\n",
      "text_model.encoder.layer.1\n",
      "text_model.encoder.layer.1.attention\n",
      "text_model.encoder.layer.1.attention.self\n",
      "text_model.encoder.layer.1.attention.self.query\n",
      "text_model.encoder.layer.1.attention.self.key\n",
      "text_model.encoder.layer.1.attention.self.value\n",
      "text_model.encoder.layer.1.attention.self.dropout\n",
      "text_model.encoder.layer.1.attention.output\n",
      "text_model.encoder.layer.1.attention.output.dense\n",
      "text_model.encoder.layer.1.attention.output.LayerNorm\n",
      "text_model.encoder.layer.1.attention.output.dropout\n",
      "text_model.encoder.layer.1.crossattention\n",
      "text_model.encoder.layer.1.crossattention.self\n",
      "text_model.encoder.layer.1.crossattention.self.query\n",
      "text_model.encoder.layer.1.crossattention.self.key\n",
      "text_model.encoder.layer.1.crossattention.self.value\n",
      "text_model.encoder.layer.1.crossattention.self.dropout\n",
      "text_model.encoder.layer.1.crossattention.output\n",
      "text_model.encoder.layer.1.crossattention.output.dense\n",
      "text_model.encoder.layer.1.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.1.crossattention.output.dropout\n",
      "text_model.encoder.layer.1.intermediate\n",
      "text_model.encoder.layer.1.intermediate.dense\n",
      "text_model.encoder.layer.1.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.1.output\n",
      "text_model.encoder.layer.1.output.dense\n",
      "text_model.encoder.layer.1.output.LayerNorm\n",
      "text_model.encoder.layer.1.output.dropout\n",
      "text_model.encoder.layer.2\n",
      "text_model.encoder.layer.2.attention\n",
      "text_model.encoder.layer.2.attention.self\n",
      "text_model.encoder.layer.2.attention.self.query\n",
      "text_model.encoder.layer.2.attention.self.key\n",
      "text_model.encoder.layer.2.attention.self.value\n",
      "text_model.encoder.layer.2.attention.self.dropout\n",
      "text_model.encoder.layer.2.attention.output\n",
      "text_model.encoder.layer.2.attention.output.dense\n",
      "text_model.encoder.layer.2.attention.output.LayerNorm\n",
      "text_model.encoder.layer.2.attention.output.dropout\n",
      "text_model.encoder.layer.2.crossattention\n",
      "text_model.encoder.layer.2.crossattention.self\n",
      "text_model.encoder.layer.2.crossattention.self.query\n",
      "text_model.encoder.layer.2.crossattention.self.key\n",
      "text_model.encoder.layer.2.crossattention.self.value\n",
      "text_model.encoder.layer.2.crossattention.self.dropout\n",
      "text_model.encoder.layer.2.crossattention.output\n",
      "text_model.encoder.layer.2.crossattention.output.dense\n",
      "text_model.encoder.layer.2.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.2.crossattention.output.dropout\n",
      "text_model.encoder.layer.2.intermediate\n",
      "text_model.encoder.layer.2.intermediate.dense\n",
      "text_model.encoder.layer.2.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.2.output\n",
      "text_model.encoder.layer.2.output.dense\n",
      "text_model.encoder.layer.2.output.LayerNorm\n",
      "text_model.encoder.layer.2.output.dropout\n",
      "text_model.encoder.layer.3\n",
      "text_model.encoder.layer.3.attention\n",
      "text_model.encoder.layer.3.attention.self\n",
      "text_model.encoder.layer.3.attention.self.query\n",
      "text_model.encoder.layer.3.attention.self.key\n",
      "text_model.encoder.layer.3.attention.self.value\n",
      "text_model.encoder.layer.3.attention.self.dropout\n",
      "text_model.encoder.layer.3.attention.output\n",
      "text_model.encoder.layer.3.attention.output.dense\n",
      "text_model.encoder.layer.3.attention.output.LayerNorm\n",
      "text_model.encoder.layer.3.attention.output.dropout\n",
      "text_model.encoder.layer.3.crossattention\n",
      "text_model.encoder.layer.3.crossattention.self\n",
      "text_model.encoder.layer.3.crossattention.self.query\n",
      "text_model.encoder.layer.3.crossattention.self.key\n",
      "text_model.encoder.layer.3.crossattention.self.value\n",
      "text_model.encoder.layer.3.crossattention.self.dropout\n",
      "text_model.encoder.layer.3.crossattention.output\n",
      "text_model.encoder.layer.3.crossattention.output.dense\n",
      "text_model.encoder.layer.3.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.3.crossattention.output.dropout\n",
      "text_model.encoder.layer.3.intermediate\n",
      "text_model.encoder.layer.3.intermediate.dense\n",
      "text_model.encoder.layer.3.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.3.output\n",
      "text_model.encoder.layer.3.output.dense\n",
      "text_model.encoder.layer.3.output.LayerNorm\n",
      "text_model.encoder.layer.3.output.dropout\n",
      "text_model.encoder.layer.4\n",
      "text_model.encoder.layer.4.attention\n",
      "text_model.encoder.layer.4.attention.self\n",
      "text_model.encoder.layer.4.attention.self.query\n",
      "text_model.encoder.layer.4.attention.self.key\n",
      "text_model.encoder.layer.4.attention.self.value\n",
      "text_model.encoder.layer.4.attention.self.dropout\n",
      "text_model.encoder.layer.4.attention.output\n",
      "text_model.encoder.layer.4.attention.output.dense\n",
      "text_model.encoder.layer.4.attention.output.LayerNorm\n",
      "text_model.encoder.layer.4.attention.output.dropout\n",
      "text_model.encoder.layer.4.crossattention\n",
      "text_model.encoder.layer.4.crossattention.self\n",
      "text_model.encoder.layer.4.crossattention.self.query\n",
      "text_model.encoder.layer.4.crossattention.self.key\n",
      "text_model.encoder.layer.4.crossattention.self.value\n",
      "text_model.encoder.layer.4.crossattention.self.dropout\n",
      "text_model.encoder.layer.4.crossattention.output\n",
      "text_model.encoder.layer.4.crossattention.output.dense\n",
      "text_model.encoder.layer.4.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.4.crossattention.output.dropout\n",
      "text_model.encoder.layer.4.intermediate\n",
      "text_model.encoder.layer.4.intermediate.dense\n",
      "text_model.encoder.layer.4.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.4.output\n",
      "text_model.encoder.layer.4.output.dense\n",
      "text_model.encoder.layer.4.output.LayerNorm\n",
      "text_model.encoder.layer.4.output.dropout\n",
      "text_model.encoder.layer.5\n",
      "text_model.encoder.layer.5.attention\n",
      "text_model.encoder.layer.5.attention.self\n",
      "text_model.encoder.layer.5.attention.self.query\n",
      "text_model.encoder.layer.5.attention.self.key\n",
      "text_model.encoder.layer.5.attention.self.value\n",
      "text_model.encoder.layer.5.attention.self.dropout\n",
      "text_model.encoder.layer.5.attention.output\n",
      "text_model.encoder.layer.5.attention.output.dense\n",
      "text_model.encoder.layer.5.attention.output.LayerNorm\n",
      "text_model.encoder.layer.5.attention.output.dropout\n",
      "text_model.encoder.layer.5.crossattention\n",
      "text_model.encoder.layer.5.crossattention.self\n",
      "text_model.encoder.layer.5.crossattention.self.query\n",
      "text_model.encoder.layer.5.crossattention.self.key\n",
      "text_model.encoder.layer.5.crossattention.self.value\n",
      "text_model.encoder.layer.5.crossattention.self.dropout\n",
      "text_model.encoder.layer.5.crossattention.output\n",
      "text_model.encoder.layer.5.crossattention.output.dense\n",
      "text_model.encoder.layer.5.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.5.crossattention.output.dropout\n",
      "text_model.encoder.layer.5.intermediate\n",
      "text_model.encoder.layer.5.intermediate.dense\n",
      "text_model.encoder.layer.5.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.5.output\n",
      "text_model.encoder.layer.5.output.dense\n",
      "text_model.encoder.layer.5.output.LayerNorm\n",
      "text_model.encoder.layer.5.output.dropout\n",
      "text_model.encoder.layer.6\n",
      "text_model.encoder.layer.6.attention\n",
      "text_model.encoder.layer.6.attention.self\n",
      "text_model.encoder.layer.6.attention.self.query\n",
      "text_model.encoder.layer.6.attention.self.key\n",
      "text_model.encoder.layer.6.attention.self.value\n",
      "text_model.encoder.layer.6.attention.self.dropout\n",
      "text_model.encoder.layer.6.attention.output\n",
      "text_model.encoder.layer.6.attention.output.dense\n",
      "text_model.encoder.layer.6.attention.output.LayerNorm\n",
      "text_model.encoder.layer.6.attention.output.dropout\n",
      "text_model.encoder.layer.6.crossattention\n",
      "text_model.encoder.layer.6.crossattention.self\n",
      "text_model.encoder.layer.6.crossattention.self.query\n",
      "text_model.encoder.layer.6.crossattention.self.key\n",
      "text_model.encoder.layer.6.crossattention.self.value\n",
      "text_model.encoder.layer.6.crossattention.self.dropout\n",
      "text_model.encoder.layer.6.crossattention.output\n",
      "text_model.encoder.layer.6.crossattention.output.dense\n",
      "text_model.encoder.layer.6.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.6.crossattention.output.dropout\n",
      "text_model.encoder.layer.6.intermediate\n",
      "text_model.encoder.layer.6.intermediate.dense\n",
      "text_model.encoder.layer.6.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.6.output\n",
      "text_model.encoder.layer.6.output.dense\n",
      "text_model.encoder.layer.6.output.LayerNorm\n",
      "text_model.encoder.layer.6.output.dropout\n",
      "text_model.encoder.layer.7\n",
      "text_model.encoder.layer.7.attention\n",
      "text_model.encoder.layer.7.attention.self\n",
      "text_model.encoder.layer.7.attention.self.query\n",
      "text_model.encoder.layer.7.attention.self.key\n",
      "text_model.encoder.layer.7.attention.self.value\n",
      "text_model.encoder.layer.7.attention.self.dropout\n",
      "text_model.encoder.layer.7.attention.output\n",
      "text_model.encoder.layer.7.attention.output.dense\n",
      "text_model.encoder.layer.7.attention.output.LayerNorm\n",
      "text_model.encoder.layer.7.attention.output.dropout\n",
      "text_model.encoder.layer.7.crossattention\n",
      "text_model.encoder.layer.7.crossattention.self\n",
      "text_model.encoder.layer.7.crossattention.self.query\n",
      "text_model.encoder.layer.7.crossattention.self.key\n",
      "text_model.encoder.layer.7.crossattention.self.value\n",
      "text_model.encoder.layer.7.crossattention.self.dropout\n",
      "text_model.encoder.layer.7.crossattention.output\n",
      "text_model.encoder.layer.7.crossattention.output.dense\n",
      "text_model.encoder.layer.7.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.7.crossattention.output.dropout\n",
      "text_model.encoder.layer.7.intermediate\n",
      "text_model.encoder.layer.7.intermediate.dense\n",
      "text_model.encoder.layer.7.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.7.output\n",
      "text_model.encoder.layer.7.output.dense\n",
      "text_model.encoder.layer.7.output.LayerNorm\n",
      "text_model.encoder.layer.7.output.dropout\n",
      "text_model.encoder.layer.8\n",
      "text_model.encoder.layer.8.attention\n",
      "text_model.encoder.layer.8.attention.self\n",
      "text_model.encoder.layer.8.attention.self.query\n",
      "text_model.encoder.layer.8.attention.self.key\n",
      "text_model.encoder.layer.8.attention.self.value\n",
      "text_model.encoder.layer.8.attention.self.dropout\n",
      "text_model.encoder.layer.8.attention.output\n",
      "text_model.encoder.layer.8.attention.output.dense\n",
      "text_model.encoder.layer.8.attention.output.LayerNorm\n",
      "text_model.encoder.layer.8.attention.output.dropout\n",
      "text_model.encoder.layer.8.crossattention\n",
      "text_model.encoder.layer.8.crossattention.self\n",
      "text_model.encoder.layer.8.crossattention.self.query\n",
      "text_model.encoder.layer.8.crossattention.self.key\n",
      "text_model.encoder.layer.8.crossattention.self.value\n",
      "text_model.encoder.layer.8.crossattention.self.dropout\n",
      "text_model.encoder.layer.8.crossattention.output\n",
      "text_model.encoder.layer.8.crossattention.output.dense\n",
      "text_model.encoder.layer.8.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.8.crossattention.output.dropout\n",
      "text_model.encoder.layer.8.intermediate\n",
      "text_model.encoder.layer.8.intermediate.dense\n",
      "text_model.encoder.layer.8.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.8.output\n",
      "text_model.encoder.layer.8.output.dense\n",
      "text_model.encoder.layer.8.output.LayerNorm\n",
      "text_model.encoder.layer.8.output.dropout\n",
      "text_model.encoder.layer.9\n",
      "text_model.encoder.layer.9.attention\n",
      "text_model.encoder.layer.9.attention.self\n",
      "text_model.encoder.layer.9.attention.self.query\n",
      "text_model.encoder.layer.9.attention.self.key\n",
      "text_model.encoder.layer.9.attention.self.value\n",
      "text_model.encoder.layer.9.attention.self.dropout\n",
      "text_model.encoder.layer.9.attention.output\n",
      "text_model.encoder.layer.9.attention.output.dense\n",
      "text_model.encoder.layer.9.attention.output.LayerNorm\n",
      "text_model.encoder.layer.9.attention.output.dropout\n",
      "text_model.encoder.layer.9.crossattention\n",
      "text_model.encoder.layer.9.crossattention.self\n",
      "text_model.encoder.layer.9.crossattention.self.query\n",
      "text_model.encoder.layer.9.crossattention.self.key\n",
      "text_model.encoder.layer.9.crossattention.self.value\n",
      "text_model.encoder.layer.9.crossattention.self.dropout\n",
      "text_model.encoder.layer.9.crossattention.output\n",
      "text_model.encoder.layer.9.crossattention.output.dense\n",
      "text_model.encoder.layer.9.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.9.crossattention.output.dropout\n",
      "text_model.encoder.layer.9.intermediate\n",
      "text_model.encoder.layer.9.intermediate.dense\n",
      "text_model.encoder.layer.9.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.9.output\n",
      "text_model.encoder.layer.9.output.dense\n",
      "text_model.encoder.layer.9.output.LayerNorm\n",
      "text_model.encoder.layer.9.output.dropout\n",
      "text_model.encoder.layer.10\n",
      "text_model.encoder.layer.10.attention\n",
      "text_model.encoder.layer.10.attention.self\n",
      "text_model.encoder.layer.10.attention.self.query\n",
      "text_model.encoder.layer.10.attention.self.key\n",
      "text_model.encoder.layer.10.attention.self.value\n",
      "text_model.encoder.layer.10.attention.self.dropout\n",
      "text_model.encoder.layer.10.attention.output\n",
      "text_model.encoder.layer.10.attention.output.dense\n",
      "text_model.encoder.layer.10.attention.output.LayerNorm\n",
      "text_model.encoder.layer.10.attention.output.dropout\n",
      "text_model.encoder.layer.10.crossattention\n",
      "text_model.encoder.layer.10.crossattention.self\n",
      "text_model.encoder.layer.10.crossattention.self.query\n",
      "text_model.encoder.layer.10.crossattention.self.key\n",
      "text_model.encoder.layer.10.crossattention.self.value\n",
      "text_model.encoder.layer.10.crossattention.self.dropout\n",
      "text_model.encoder.layer.10.crossattention.output\n",
      "text_model.encoder.layer.10.crossattention.output.dense\n",
      "text_model.encoder.layer.10.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.10.crossattention.output.dropout\n",
      "text_model.encoder.layer.10.intermediate\n",
      "text_model.encoder.layer.10.intermediate.dense\n",
      "text_model.encoder.layer.10.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.10.output\n",
      "text_model.encoder.layer.10.output.dense\n",
      "text_model.encoder.layer.10.output.LayerNorm\n",
      "text_model.encoder.layer.10.output.dropout\n",
      "text_model.encoder.layer.11\n",
      "text_model.encoder.layer.11.attention\n",
      "text_model.encoder.layer.11.attention.self\n",
      "text_model.encoder.layer.11.attention.self.query\n",
      "text_model.encoder.layer.11.attention.self.key\n",
      "text_model.encoder.layer.11.attention.self.value\n",
      "text_model.encoder.layer.11.attention.self.dropout\n",
      "text_model.encoder.layer.11.attention.output\n",
      "text_model.encoder.layer.11.attention.output.dense\n",
      "text_model.encoder.layer.11.attention.output.LayerNorm\n",
      "text_model.encoder.layer.11.attention.output.dropout\n",
      "text_model.encoder.layer.11.crossattention\n",
      "text_model.encoder.layer.11.crossattention.self\n",
      "text_model.encoder.layer.11.crossattention.self.query\n",
      "text_model.encoder.layer.11.crossattention.self.key\n",
      "text_model.encoder.layer.11.crossattention.self.value\n",
      "text_model.encoder.layer.11.crossattention.self.dropout\n",
      "text_model.encoder.layer.11.crossattention.output\n",
      "text_model.encoder.layer.11.crossattention.output.dense\n",
      "text_model.encoder.layer.11.crossattention.output.LayerNorm\n",
      "text_model.encoder.layer.11.crossattention.output.dropout\n",
      "text_model.encoder.layer.11.intermediate\n",
      "text_model.encoder.layer.11.intermediate.dense\n",
      "text_model.encoder.layer.11.intermediate.intermediate_act_fn\n",
      "text_model.encoder.layer.11.output\n",
      "text_model.encoder.layer.11.output.dense\n",
      "text_model.encoder.layer.11.output.LayerNorm\n",
      "text_model.encoder.layer.11.output.dropout\n",
      "text_model.pooler\n",
      "text_model.pooler.dense\n",
      "text_model.pooler.activation\n",
      "vision_model\n",
      "vision_model.embeddings\n",
      "vision_model.embeddings.patch_embedding\n",
      "vision_model.encoder\n",
      "vision_model.encoder.layers\n",
      "vision_model.encoder.layers.0\n",
      "vision_model.encoder.layers.0.self_attn\n",
      "vision_model.encoder.layers.0.self_attn.dropout\n",
      "vision_model.encoder.layers.0.self_attn.qkv\n",
      "vision_model.encoder.layers.0.self_attn.projection\n",
      "vision_model.encoder.layers.0.layer_norm1\n",
      "vision_model.encoder.layers.0.mlp\n",
      "vision_model.encoder.layers.0.mlp.activation_fn\n",
      "vision_model.encoder.layers.0.mlp.fc1\n",
      "vision_model.encoder.layers.0.mlp.fc2\n",
      "vision_model.encoder.layers.0.layer_norm2\n",
      "vision_model.encoder.layers.1\n",
      "vision_model.encoder.layers.1.self_attn\n",
      "vision_model.encoder.layers.1.self_attn.dropout\n",
      "vision_model.encoder.layers.1.self_attn.qkv\n",
      "vision_model.encoder.layers.1.self_attn.projection\n",
      "vision_model.encoder.layers.1.layer_norm1\n",
      "vision_model.encoder.layers.1.mlp\n",
      "vision_model.encoder.layers.1.mlp.activation_fn\n",
      "vision_model.encoder.layers.1.mlp.fc1\n",
      "vision_model.encoder.layers.1.mlp.fc2\n",
      "vision_model.encoder.layers.1.layer_norm2\n",
      "vision_model.encoder.layers.2\n",
      "vision_model.encoder.layers.2.self_attn\n",
      "vision_model.encoder.layers.2.self_attn.dropout\n",
      "vision_model.encoder.layers.2.self_attn.qkv\n",
      "vision_model.encoder.layers.2.self_attn.projection\n",
      "vision_model.encoder.layers.2.layer_norm1\n",
      "vision_model.encoder.layers.2.mlp\n",
      "vision_model.encoder.layers.2.mlp.activation_fn\n",
      "vision_model.encoder.layers.2.mlp.fc1\n",
      "vision_model.encoder.layers.2.mlp.fc2\n",
      "vision_model.encoder.layers.2.layer_norm2\n",
      "vision_model.encoder.layers.3\n",
      "vision_model.encoder.layers.3.self_attn\n",
      "vision_model.encoder.layers.3.self_attn.dropout\n",
      "vision_model.encoder.layers.3.self_attn.qkv\n",
      "vision_model.encoder.layers.3.self_attn.projection\n",
      "vision_model.encoder.layers.3.layer_norm1\n",
      "vision_model.encoder.layers.3.mlp\n",
      "vision_model.encoder.layers.3.mlp.activation_fn\n",
      "vision_model.encoder.layers.3.mlp.fc1\n",
      "vision_model.encoder.layers.3.mlp.fc2\n",
      "vision_model.encoder.layers.3.layer_norm2\n",
      "vision_model.encoder.layers.4\n",
      "vision_model.encoder.layers.4.self_attn\n",
      "vision_model.encoder.layers.4.self_attn.dropout\n",
      "vision_model.encoder.layers.4.self_attn.qkv\n",
      "vision_model.encoder.layers.4.self_attn.projection\n",
      "vision_model.encoder.layers.4.layer_norm1\n",
      "vision_model.encoder.layers.4.mlp\n",
      "vision_model.encoder.layers.4.mlp.activation_fn\n",
      "vision_model.encoder.layers.4.mlp.fc1\n",
      "vision_model.encoder.layers.4.mlp.fc2\n",
      "vision_model.encoder.layers.4.layer_norm2\n",
      "vision_model.encoder.layers.5\n",
      "vision_model.encoder.layers.5.self_attn\n",
      "vision_model.encoder.layers.5.self_attn.dropout\n",
      "vision_model.encoder.layers.5.self_attn.qkv\n",
      "vision_model.encoder.layers.5.self_attn.projection\n",
      "vision_model.encoder.layers.5.layer_norm1\n",
      "vision_model.encoder.layers.5.mlp\n",
      "vision_model.encoder.layers.5.mlp.activation_fn\n",
      "vision_model.encoder.layers.5.mlp.fc1\n",
      "vision_model.encoder.layers.5.mlp.fc2\n",
      "vision_model.encoder.layers.5.layer_norm2\n",
      "vision_model.encoder.layers.6\n",
      "vision_model.encoder.layers.6.self_attn\n",
      "vision_model.encoder.layers.6.self_attn.dropout\n",
      "vision_model.encoder.layers.6.self_attn.qkv\n",
      "vision_model.encoder.layers.6.self_attn.projection\n",
      "vision_model.encoder.layers.6.layer_norm1\n",
      "vision_model.encoder.layers.6.mlp\n",
      "vision_model.encoder.layers.6.mlp.activation_fn\n",
      "vision_model.encoder.layers.6.mlp.fc1\n",
      "vision_model.encoder.layers.6.mlp.fc2\n",
      "vision_model.encoder.layers.6.layer_norm2\n",
      "vision_model.encoder.layers.7\n",
      "vision_model.encoder.layers.7.self_attn\n",
      "vision_model.encoder.layers.7.self_attn.dropout\n",
      "vision_model.encoder.layers.7.self_attn.qkv\n",
      "vision_model.encoder.layers.7.self_attn.projection\n",
      "vision_model.encoder.layers.7.layer_norm1\n",
      "vision_model.encoder.layers.7.mlp\n",
      "vision_model.encoder.layers.7.mlp.activation_fn\n",
      "vision_model.encoder.layers.7.mlp.fc1\n",
      "vision_model.encoder.layers.7.mlp.fc2\n",
      "vision_model.encoder.layers.7.layer_norm2\n",
      "vision_model.encoder.layers.8\n",
      "vision_model.encoder.layers.8.self_attn\n",
      "vision_model.encoder.layers.8.self_attn.dropout\n",
      "vision_model.encoder.layers.8.self_attn.qkv\n",
      "vision_model.encoder.layers.8.self_attn.projection\n",
      "vision_model.encoder.layers.8.layer_norm1\n",
      "vision_model.encoder.layers.8.mlp\n",
      "vision_model.encoder.layers.8.mlp.activation_fn\n",
      "vision_model.encoder.layers.8.mlp.fc1\n",
      "vision_model.encoder.layers.8.mlp.fc2\n",
      "vision_model.encoder.layers.8.layer_norm2\n",
      "vision_model.encoder.layers.9\n",
      "vision_model.encoder.layers.9.self_attn\n",
      "vision_model.encoder.layers.9.self_attn.dropout\n",
      "vision_model.encoder.layers.9.self_attn.qkv\n",
      "vision_model.encoder.layers.9.self_attn.projection\n",
      "vision_model.encoder.layers.9.layer_norm1\n",
      "vision_model.encoder.layers.9.mlp\n",
      "vision_model.encoder.layers.9.mlp.activation_fn\n",
      "vision_model.encoder.layers.9.mlp.fc1\n",
      "vision_model.encoder.layers.9.mlp.fc2\n",
      "vision_model.encoder.layers.9.layer_norm2\n",
      "vision_model.encoder.layers.10\n",
      "vision_model.encoder.layers.10.self_attn\n",
      "vision_model.encoder.layers.10.self_attn.dropout\n",
      "vision_model.encoder.layers.10.self_attn.qkv\n",
      "vision_model.encoder.layers.10.self_attn.projection\n",
      "vision_model.encoder.layers.10.layer_norm1\n",
      "vision_model.encoder.layers.10.mlp\n",
      "vision_model.encoder.layers.10.mlp.activation_fn\n",
      "vision_model.encoder.layers.10.mlp.fc1\n",
      "vision_model.encoder.layers.10.mlp.fc2\n",
      "vision_model.encoder.layers.10.layer_norm2\n",
      "vision_model.encoder.layers.11\n",
      "vision_model.encoder.layers.11.self_attn\n",
      "vision_model.encoder.layers.11.self_attn.dropout\n",
      "vision_model.encoder.layers.11.self_attn.qkv\n",
      "vision_model.encoder.layers.11.self_attn.projection\n",
      "vision_model.encoder.layers.11.layer_norm1\n",
      "vision_model.encoder.layers.11.mlp\n",
      "vision_model.encoder.layers.11.mlp.activation_fn\n",
      "vision_model.encoder.layers.11.mlp.fc1\n",
      "vision_model.encoder.layers.11.mlp.fc2\n",
      "vision_model.encoder.layers.11.layer_norm2\n",
      "vision_model.post_layernorm\n",
      "visual_projection\n",
      "text_projection\n"
     ]
    }
   ],
   "source": [
    "for name, module in blip_model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ubuntu/vqa/huggingface_modules/models--google--mt5-small/snapshots/38f23af8ec210eb6c376d40e9c56bd25a80f195d were not used when initializing MT5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--google--mt5-small/snapshots/38f23af8ec210eb6c376d40e9c56bd25a80f195d\")\n",
    "t5_model = AutoModel.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--google--mt5-small/snapshots/38f23af8ec210eb6c376d40e9c56bd25a80f195d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(250112, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.shared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/vqa/test_model.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdgx_lyson/home/ubuntu/vqa/test_model.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m([(key, value\u001b[39m.\u001b[39mcuda(cuda_id)) \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(loader))\u001b[39m.\u001b[39mitems()])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx_lyson/home/ubuntu/vqa/test_model.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m generate_inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = dict([(key, value.cuda(cuda_id)) for key, value in next(iter(loader)).items()])\n",
    "generate_inputs = inputs.pop(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_outputs = vqa_model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yellow [SEP] [PAD] [PAD]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(generate_outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 50, 768]), torch.Size([32, 50]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = outputs[\"text_model_output\"].last_hidden_state\n",
    "logits.shape, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../aten/src/ATen/native/cuda/Loss.cu:240: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/IPython/core/formatters.py:707\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[1;32m    701\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    702\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[1;32m    703\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[1;32m    704\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[1;32m    705\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[1;32m    706\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 707\u001b[0m printer\u001b[39m.\u001b[39;49mpretty(obj)\n\u001b[1;32m    708\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    709\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39;49m, cycle)\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39;49m(obj)\n\u001b[1;32m    779\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/torch/_tensor.py:426\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    423\u001b[0m         Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/torch/_tensor_str.py:636\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39m_python_dispatch\u001b[39m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    635\u001b[0m     guard \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 636\u001b[0m     \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/torch/_tensor_str.py:567\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    565\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    566\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[1;32m    570\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/torch/_tensor_str.py:327\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    324\u001b[0m         \u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;49;00m summarize \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/torch/_tensor_str.py:116\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mlen\u001b[39m(value_str))\n\u001b[1;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     nonzero_finite_vals \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmasked_select(\n\u001b[0;32m--> 116\u001b[0m         tensor_view, torch\u001b[39m.\u001b[39;49misfinite(tensor_view) \u001b[39m&\u001b[39m tensor_view\u001b[39m.\u001b[39mne(\u001b[39m0\u001b[39m)\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m nonzero_finite_vals\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    120\u001b[0m         \u001b[39m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    121\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "loss_fn(logits.reshape(-1, 768), a.reshape(-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/ubuntu/vqa/huggingface_modules/models--google--mt5-small/snapshots/38f23af8ec210eb6c376d40e9c56bd25a80f195d were not used when initializing MT5Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5Model, AutoTokenizer, MT5ForConditionalGeneration\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--google--mt5-small/snapshots/38f23af8ec210eb6c376d40e9c56bd25a80f195d\")\n",
    "model = MT5Model.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--google--mt5-small/snapshots/38f23af8ec210eb6c376d40e9c56bd25a80f195d\")\n",
    "article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n",
    "summary = \"Weiter Verhandlung in Syrien.\"\n",
    "inputs = tokenizer(article, return_tensors=\"pt\")\n",
    "labels = tokenizer(summary, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(input_ids=inputs[\"input_ids\"], decoder_input_ids=labels[\"input_ids\"])\n",
    "hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.</s>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MT5ForConditionalGeneration.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--google--mt5-small/snapshots/38f23af8ec210eb6c376d40e9c56bd25a80f195d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/ubuntu/vqa/huggingface_modules/models--google--mt5-small does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/ubuntu/vqa/huggingface_modules/models--google--mt5-small/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/vqa/test_model.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx_lyson/home/ubuntu/vqa/test_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m MT5ForConditionalGeneration, AutoTokenizer\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdgx_lyson/home/ubuntu/vqa/test_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m MT5ForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m/home/ubuntu/vqa/huggingface_modules/models--google--mt5-small\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx_lyson/home/ubuntu/vqa/test_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgoogle/mt5-small\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx_lyson/home/ubuntu/vqa/test_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx_lyson/home/ubuntu/vqa/test_model.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# summary = \"Weiter Verhandlung in Syrien.\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/transformers/modeling_utils.py:2269\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2268\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2269\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   2270\u001b[0m         config_path,\n\u001b[1;32m   2271\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2272\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2273\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2274\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2275\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2276\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2277\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   2278\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2279\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2280\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   2281\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   2282\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2283\u001b[0m     )\n\u001b[1;32m   2284\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2285\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/transformers/configuration_utils.py:546\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    470\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    547\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    548\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    549\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    550\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m         )\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/transformers/configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    572\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    575\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/transformers/configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    624\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    626\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    629\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    630\u001b[0m         configuration_file,\n\u001b[1;32m    631\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    632\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    633\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    634\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    635\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    636\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    637\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    638\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    639\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    640\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    643\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/vqa/lib/python3.10/site-packages/transformers/utils/hub.py:380\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 380\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    381\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m         )\n\u001b[1;32m    384\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/ubuntu/vqa/huggingface_modules/models--google--mt5-small does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/ubuntu/vqa/huggingface_modules/models--google--mt5-small/None' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"/home/ubuntu/vqa/huggingface_modules/models--google--mt5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "# article = \"UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.\"\n",
    "# summary = \"Weiter Verhandlung in Syrien.\"\n",
    "inputs = tokenizer(article, text_target=summary, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = tokenizer(article, summary, return_tensors=\"pt\")\n",
    "inputs2 = tokenizer(article, text_target=summary, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs1.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.</s> Weiter Verhandlung in Syrien.</s>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs1.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.</s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs2.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 10969,    443, 209522,    295,    259,   8992,    261,    259,   3648,\n",
       "           8104,    672,  53764,   2100,   8497,    281,  55984,    278,    260,\n",
       "              1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tokens = model.encoder.embed_tokens(inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_bias = None\n",
    "for block in model.encoder.block:\n",
    "    embed_tokens, position_bias = block(embed_tokens, inputs.attention_mask, position_bias=position_bias)\n",
    "embed_tokens = model.encoder.final_layer_norm(embed_tokens)\n",
    "embed_tokens = model.encoder.dropout(embed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [False,  True, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [ True, False,  True,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tokens == model.encoder(inputs.input_ids, use_cache=False).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1110, -0.0287, -0.4402,  ...,  0.5384,  0.0579,  0.3085],\n",
       "          [ 0.2239, -0.3732,  0.0509,  ...,  0.3491, -0.2021,  0.0417],\n",
       "          [ 0.1450, -0.2738, -0.0957,  ...,  0.3628, -0.0667,  0.0604],\n",
       "          ...,\n",
       "          [-0.0709, -0.4040, -0.0700,  ..., -0.0504, -0.1645,  0.1855],\n",
       "          [-0.0890, -0.6575, -0.3589,  ..., -0.0446,  0.0796,  0.0199],\n",
       "          [-0.0388, -0.0067,  0.0084,  ..., -0.0243,  0.0069,  0.0217]]],\n",
       "        grad_fn=<MulBackward0>),\n",
       " tensor([[[ 0.1110, -0.0287, -0.4402,  ...,  0.5384,  0.0579,  0.3085],\n",
       "          [ 0.2239, -0.3732,  0.0509,  ...,  0.3491, -0.2021,  0.0417],\n",
       "          [ 0.1450, -0.2738, -0.0957,  ...,  0.3628, -0.0667,  0.0604],\n",
       "          ...,\n",
       "          [-0.0709, -0.4040, -0.0700,  ..., -0.0504, -0.1645,  0.1855],\n",
       "          [-0.0890, -0.6575, -0.3589,  ..., -0.0446,  0.0796,  0.0199],\n",
       "          [-0.0388, -0.0067,  0.0084,  ..., -0.0243,  0.0069,  0.0217]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tokens, model.encoder(inputs.input_ids).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5Stack(\n",
       "  (embed_tokens): Embedding(250112, 512)\n",
       "  (block): ModuleList(\n",
       "    (0): MT5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): MT5LayerSelfAttention(\n",
       "          (SelfAttention): MT5Attention(\n",
       "            (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            (relative_attention_bias): Embedding(32, 6)\n",
       "          )\n",
       "          (layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): MT5LayerFF(\n",
       "          (DenseReluDense): MT5DenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1-7): 7 x MT5Block(\n",
       "      (layer): ModuleList(\n",
       "        (0): MT5LayerSelfAttention(\n",
       "          (SelfAttention): MT5Attention(\n",
       "            (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "            (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "          )\n",
       "          (layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): MT5LayerFF(\n",
       "          (DenseReluDense): MT5DenseGatedActDense(\n",
       "            (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "            (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (act): NewGELUActivation()\n",
       "          )\n",
       "          (layer_norm): MT5LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): MT5LayerNorm()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5LayerNorm()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.final_layer_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-3.3846e+00,  1.4891e+00, -3.0494e-02,  ..., -1.6864e+01,\n",
       "           -2.2443e+01, -4.4718e+01],\n",
       "          [-5.8449e+00,  3.8363e+00,  1.9972e-01,  ..., -1.0023e+01,\n",
       "           -1.7387e+01, -4.0663e+01],\n",
       "          [-4.4669e+00,  9.1005e+00, -3.1199e+00,  ..., -1.5883e+01,\n",
       "           -1.9319e+01, -4.0251e+01],\n",
       "          ...,\n",
       "          [-1.5249e+00,  3.6194e+00,  2.7254e+00,  ..., -6.7944e+00,\n",
       "           -2.0742e+01, -4.7024e+01],\n",
       "          [ 6.6335e-02, -5.5098e+00,  7.5768e-01,  ..., -1.0822e+01,\n",
       "           -2.2477e+01, -3.8480e+01],\n",
       "          [-9.4237e+00, -1.7733e+01, -8.1069e+00,  ...,  1.0379e+01,\n",
       "            4.7321e+01,  2.8246e+01]]], grad_fn=<AddBackward0>),\n",
       " tensor([[[[  1.7422,   7.6875,   8.0000,  ...,   6.8438,   6.8438,   6.8438],\n",
       "           [  7.4375,   1.7422,   7.6875,  ...,   7.1875,   6.8438,   6.8438],\n",
       "           [  7.6250,   7.4375,   1.7422,  ...,   7.1875,   7.1875,   6.8438],\n",
       "           ...,\n",
       "           [  6.4688,   6.7812,   6.7812,  ...,   1.7422,   7.6875,   8.0000],\n",
       "           [  6.4688,   6.4688,   6.7812,  ...,   7.4375,   1.7422,   7.6875],\n",
       "           [  6.4688,   6.4688,   6.4688,  ...,   7.6250,   7.4375,   1.7422]],\n",
       " \n",
       "          [[  7.6562,  -8.6250,  -6.5625,  ..., -18.3750, -18.3750, -18.3750],\n",
       "           [ 10.8125,   7.6562,  -8.6250,  ..., -17.2500, -18.3750, -18.3750],\n",
       "           [  9.9375,  10.8125,   7.6562,  ..., -17.2500, -17.2500, -18.3750],\n",
       "           ...,\n",
       "           [  6.4062,   7.0312,   7.0312,  ...,   7.6562,  -8.6250,  -6.5625],\n",
       "           [  6.4062,   6.4062,   7.0312,  ...,  10.8125,   7.6562,  -8.6250],\n",
       "           [  6.4062,   6.4062,   6.4062,  ...,   9.9375,  10.8125,   7.6562]],\n",
       " \n",
       "          [[  9.1250,  11.1875,   9.8750,  ...,   5.3750,   5.3750,   5.3750],\n",
       "           [ -6.5938,   9.1250,  11.1875,  ...,   6.0938,   5.3750,   5.3750],\n",
       "           [ -7.2500,  -6.5938,   9.1250,  ...,   6.0938,   6.0938,   5.3750],\n",
       "           ...,\n",
       "           [ -8.1875,  -8.0625,  -8.0625,  ...,   9.1250,  11.1875,   9.8750],\n",
       "           [ -8.1875,  -8.1875,  -8.0625,  ...,  -6.5938,   9.1250,  11.1875],\n",
       "           [ -8.1875,  -8.1875,  -8.1875,  ...,  -7.2500,  -6.5938,   9.1250]],\n",
       " \n",
       "          [[ -1.4531,   3.0312,   3.9844,  ...,   5.0938,   5.0938,   5.0938],\n",
       "           [  1.9688,  -1.4531,   3.0312,  ...,   5.0625,   5.0938,   5.0938],\n",
       "           [  4.2188,   1.9688,  -1.4531,  ...,   5.0625,   5.0625,   5.0938],\n",
       "           ...,\n",
       "           [  5.3125,   5.3750,   5.3750,  ...,  -1.4531,   3.0312,   3.9844],\n",
       "           [  5.3125,   5.3125,   5.3750,  ...,   1.9688,  -1.4531,   3.0312],\n",
       "           [  5.3125,   5.3125,   5.3125,  ...,   4.2188,   1.9688,  -1.4531]],\n",
       " \n",
       "          [[ -0.2246,   2.0312,   2.0156,  ...,   2.0000,   2.0000,   2.0000],\n",
       "           [ 12.2500,  -0.2246,   2.0312,  ...,   2.0312,   2.0000,   2.0000],\n",
       "           [  9.9375,  12.2500,  -0.2246,  ...,   2.0312,   2.0312,   2.0000],\n",
       "           ...,\n",
       "           [  2.8750,   3.9844,   3.9844,  ...,  -0.2246,   2.0312,   2.0156],\n",
       "           [  2.8750,   2.8750,   3.9844,  ...,  12.2500,  -0.2246,   2.0312],\n",
       "           [  2.8750,   2.8750,   2.8750,  ...,   9.9375,  12.2500,  -0.2246]],\n",
       " \n",
       "          [[ -7.7812,   4.7500,   5.4062,  ...,   5.6875,   5.6875,   5.6875],\n",
       "           [  5.9062,  -7.7812,   4.7500,  ...,   5.7188,   5.6875,   5.6875],\n",
       "           [  6.4062,   5.9062,  -7.7812,  ...,   5.7188,   5.7188,   5.6875],\n",
       "           ...,\n",
       "           [  6.1875,   6.3125,   6.3125,  ...,  -7.7812,   4.7500,   5.4062],\n",
       "           [  6.1875,   6.1875,   6.3125,  ...,   5.9062,  -7.7812,   4.7500],\n",
       "           [  6.1875,   6.1875,   6.1875,  ...,   6.4062,   5.9062,  -7.7812]]]],\n",
       "        grad_fn=<UnsqueezeBackward0>))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.block[0](embed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4790, -0.0256, -0.3910,  ...,  0.0696,  0.1549,  0.5829],\n",
       "         [-0.1468,  0.5403, -0.3263,  ...,  0.1398, -0.0957,  0.3944],\n",
       "         [-0.0460,  0.4281, -0.2796,  ...,  0.1270,  0.0163,  0.4306],\n",
       "         ...,\n",
       "         [ 0.0908,  0.2675, -0.2022,  ...,  0.2862, -0.1342,  0.4868],\n",
       "         [-0.2392,  0.1952, -0.0180,  ...,  0.2424,  0.2441,  0.1907],\n",
       "         [-0.3405,  0.1611,  0.3945,  ..., -0.0517,  0.0780, -0.1666]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4790, -0.0256, -0.3910,  ...,  0.0696,  0.1549,  0.5829],\n",
       "         [-0.1468,  0.5403, -0.3263,  ...,  0.1398, -0.0957,  0.3944],\n",
       "         [-0.0460,  0.4281, -0.2796,  ...,  0.1270,  0.0163,  0.4306],\n",
       "         ...,\n",
       "         [ 0.0908,  0.2675, -0.2022,  ...,  0.2862, -0.1342,  0.4868],\n",
       "         [-0.2392,  0.1952, -0.0180,  ...,  0.2424,  0.2441,  0.1907],\n",
       "         [-0.3405,  0.1611,  0.3945,  ..., -0.0517,  0.0780, -0.1666]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder(input_ids=labels.input_ids,\n",
    "              encoder_hidden_states=embed_tokens).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad>'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chitb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
